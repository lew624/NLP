大家好，我是程序锅。

github上的代码封装程度高，不利于小白学习入门。

常规的大模型RAG框架有langchain等，但是langchain等框架源码理解困难，debug源码上手难度大。

因此，我写了一个人人都能看懂、人人都能修改的大模型RAG框架代码。

我将分5个章节来介绍它，分别为：

1. 如何调用大模型API

**2. RAG背景介绍**

3. 依赖环境安装

4. 知识库构建

5. 基于知识库的大模型问答

本篇文章讲**RAG背景介绍**。

## 一、为什么要用RAG？

LLM会产生误导性的 “幻觉”，训练数据会过时，处理特定知识时效率不高，缺乏专业领域的深度洞察，同时在推理能力上也有所欠缺。

正是在这样的背景下，检索增强生成技术（Retrieval-Augmented Generation,RAG）应时而生，成为大模型时代的一大趋势。

RAG通过在语言模型生成答案之前，先从广泛的专业文档数据库中检索相关信息，然后利用这些专业信息来引导大模型生成的结果，极大地提升了内容的准确性和相关性。

RAG有效地缓解了幻觉问题，提高了知识更新的速度（大模型+搜索引擎），并增强了内容生成的可追溯性，使得大型语言模型在实际应用中变得更加实用和可信。

## 二、RAG的技术路线

RAG整体技术路线可分为3大块8个小点见图1，其中包含知识库构建、知识检索和知识问答。其中核心在于**知识库构建**。


![图1.RAG技术路线](https://files.mdnice.com/user/30915/9a8d6f6f-e86e-4b0a-8e6e-1d2fdee335f9.png)


### 知识库的构建

由图1可知涉及4个小点，分别为：文件预处理、文件切分、向量化和构建索引。

#### 1.文件预处理

输入：原始文件（比如pdf、word等非结构化数据）

输出：纯文本

技术手段：pdf、word解析器，也涉及表格转文本、图片OCR等技术。



#### 2.文件切分

文件预处理后，由于大模型的输入token的限制（因为不可能一次性将所有文件，一次输入大模型），需要对文本分段。

技术手段：一般采用识别\n字符的方式分段。

```python
 lines = text.split('\n')
```

#### 3.向量化

将文本、图像、音频和视频等转化为向量矩阵的过程，也就是变成计算机可以理解的格式。

输入：分段后的文本

输出：向量化后的向量组

技术手段：通过远程调用智谱、OpenAI的embedding模型。

输出结果展示：


![image](https://files.mdnice.com/user/30915/bb6d6497-710d-4550-9bc7-9854201d0646.png)



#### 4.构建索引

段落向量化后，我们需要构建索引，为后续快速向量匹配打下基础。当然，我们也可以不构建索引，写一个for循环挨个匹配，消耗时间长一点而已。

后续这个小项目没有设置索引，大家如何感兴趣可以引入ES、向量数据库等工具构建索引。


### 向量匹配

由图1可知向量匹配涉及2个小点，分别为：向量化、知识库检索。

知识库构建完成后，用户输入一个问题，需要对用户的问题向量化。这里的向量化需要和上面知识库构建里面的向量化所使用到的模型保持一致。

一般根据实际需求，会匹配到top k个结果。k是一个超参数，用户提供。

匹配的算法一般采用欧氏距离、曼哈顿距离、余弦等。

后续小项目中，我们采用最简单的余弦相似度来衡量向量之间的相似度。

余弦相似度公式：

![余弦相似度公式](https://files.mdnice.com/user/30915/5582ad75-ea9e-47ca-bca0-01ba9f9d15b7.png)


```python
  def cosine_similarity(cls, vector1: List[float], vector2: List[float]) -> float:
        """
        calculate cosine similarity between two vectors
        """
        dot_product = np.dot(vector1, vector2)
        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)
        if not magnitude:
            return 0
        return dot_product / magnitude
```

### 知识问答

由图1可知涉及2个小点，分别为：调用提示工程模板、调用大模型LLM。

#### 1. 提示工程模板设计

输入：用户输入的问题、知识库中得到top k个相关的段落。

输出：结合提示工程模板，将用户输入的问题、知识库中得到top k个相关的段落组成一个完成的context。

```python
RAG_PROMPT_TEMPALTE="""使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。
        问题: {question}
        可参考的上下文：
        ···
        {context}
        ···
        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。
        有用的回答:"""
```

其中question就是用户问题，context是top k个相关的段落。


#### 2. 大模型调用

这里在01.如何调用大模型API有详细描述。

## 三、最后

相信通过上面的介绍，大家对为什么要用RAG、RAG的技术路线有了大概了解。下一篇文章，我们将开始环境部署，准备开发自己的大模型RAG应用。